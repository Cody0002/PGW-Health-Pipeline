{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eba4aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1421d803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.api_core.exceptions import NotFound\n",
    "import os\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# --- Google Sheets Dependencies ---\n",
    "import gspread\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from datetime import datetime\n",
    "from google.oauth2.service_account import Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b765e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Career\\Working\\KZ GROUP\\Data Synchronization\\PGW Health Dashboard\\venv\\Lib\\site-packages\\google\\auth\\_default.py:108: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "d:\\Career\\Working\\KZ GROUP\\Data Synchronization\\PGW Health Dashboard\\venv\\Lib\\site-packages\\google\\auth\\_default.py:108: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing FULL historical query (first run)...\n",
      "Query returned 929355 rows (full load).\n"
     ]
    }
   ],
   "source": [
    "# --- GLOBAL CONFIGURATION (Update these variables) ---\n",
    "CREDENTIALS_FILE = 'gen-lang-client-0209575391-96d90a513b0b.json'  # Path to your service account JSON key\n",
    "SHEET_ID = '1Nq9u4bg0tvLnUutVh2TcxXOxe-G2E65kxy_pbJ8pce4'          # The ID of your target Google Sheet\n",
    "WORKSHEET_DEPOSIT_NAME = 'Deposit Data'\n",
    "WORKSHEET_WITHDRAW_NAME = 'Withdrawal Data'\n",
    "# -----------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- FULL LOAD QUERY FUNCTION (FIRST RUN) ---\n",
    "\n",
    "def run_full_query_and_save(output_filename='daily_funding_full.csv'):\n",
    "    \"\"\"\n",
    "    Runs a BigQuery query for a FULL historical load (no watermark),\n",
    "    saves the result to a local CSV (overwrite), and returns the DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    sql_query = \"\"\"\n",
    "      -- BigQuery SQL\n",
    " WITH raw_data AS (\n",
    "    SELECT\n",
    "      f.type, f.createdAt, f.completedAt, f.providerKey, f.method, f.status, f.reqCurrency, \n",
    "      f.accountId, f.netAmount, f.insertedAt, \n",
    "      SPLIT(f.method, '/')[SAFE_OFFSET(1)] AS channel_type,\n",
    "\n",
    "      -- mapping fields\n",
    "      a.name AS brand,\n",
    "\n",
    "      DATETIME(f.createdAt, CASE f.reqCurrency\n",
    "        WHEN 'BDT' THEN '+06:00'\n",
    "        WHEN 'THB' THEN 'Asia/Bangkok'\n",
    "        WHEN 'MXN' THEN 'America/Mexico_City'\n",
    "        WHEN 'IDR' THEN 'Asia/Jakarta'\n",
    "        WHEN 'BRL' THEN 'America/Sao_Paulo'\n",
    "        WHEN 'PKR' THEN 'Asia/Karachi'\n",
    "        WHEN 'INR' THEN '+05:30'\n",
    "        WHEN 'PHP' THEN 'Asia/Manila'\n",
    "        ELSE 'UTC'\n",
    "      END) AS local_ts,\n",
    "\n",
    "      CASE \n",
    "        WHEN f.status = 'completed' AND f.type = 'deposit' THEN\n",
    "          LEAST(TIMESTAMP_DIFF(f.completedAt, f.createdAt, SECOND), 900)\n",
    "        WHEN f.status = 'completed' AND f.type = 'withdraw' THEN\n",
    "          TIMESTAMP_DIFF(f.completedAt, f.createdAt, SECOND)\n",
    "        ELSE NULL\n",
    "      END AS transaction_time\n",
    "\n",
    "    FROM `kz-dp-prod.kz_pg_to_bq_realtime.ext_funding_tx` AS f\n",
    "    LEFT JOIN `kz-dp-prod.kz_pg_to_bq_realtime.account` AS a ON f.accountId = a.id\n",
    "    WHERE \n",
    "      DATE(f.insertedAt) > '2025-11-01'\n",
    "      AND f.type IN ('deposit', 'withdraw')\n",
    "      AND f.reqCurrency IN ('BDT', 'THB', 'MXN', 'IDR', 'BRL', 'PKR', 'INR', 'PHP')\n",
    "      AND f.status IN ('completed', 'errors', 'timeout', 'error')\n",
    "    \n",
    "    QUALIFY ROW_NUMBER() OVER (PARTITION BY f.id ORDER BY f.updatedAt DESC) = 1\n",
    "  ),\n",
    "\n",
    "      -- 2. Enrich Dimensions & Join Account Info (No change needed here)\n",
    "    all_transactions AS (\n",
    "    SELECT\n",
    "      r.*,\n",
    "      CASE WHEN LEFT(a.group,3) = 'kzg' THEN 'KZG' ELSE 'KZP' END AS group_re,\n",
    "      UPPER(a.group) AS account_group,\n",
    "      DATE(r.local_ts) AS transaction_date,\n",
    "      DATE_TRUNC(DATE(r.local_ts), MONTH) AS transaction_month,\n",
    "      FORMAT_DATETIME('%H:00 - %H:59', r.local_ts) AS Hour,\n",
    "      UPPER(r.type) AS type_formatted, \n",
    "      CASE WHEN r.status = 'errors' THEN 'error' ELSE r.status END AS status_formatted\n",
    "    FROM raw_data r\n",
    "    LEFT JOIN `kz-dp-prod.kz_pg_to_bq_realtime.account` a \n",
    "      ON r.accountId = a.Id\n",
    "  ),\n",
    "      -- 3. Calculate Quantile Bounds (DAILY Stats) - Only for the new/updated transactions\n",
    "      quantile_stats AS (\n",
    "        SELECT\n",
    "          transaction_date,\n",
    "          providerKey,\n",
    "          method,\n",
    "          channel_type,\n",
    "          type_formatted,\n",
    "          reqCurrency,\n",
    "          Hour,\n",
    "          APPROX_QUANTILES(transaction_time, 101)[OFFSET(5)] AS p05,\n",
    "          APPROX_QUANTILES(transaction_time, 101)[OFFSET(50)] AS p50,\n",
    "          APPROX_QUANTILES(transaction_time, 101)[OFFSET(95)] AS p95\n",
    "        FROM all_transactions\n",
    "        WHERE status_formatted = 'completed' AND transaction_time IS NOT NULL\n",
    "        GROUP BY 1, 2, 3, 4, 5, 6, 7\n",
    "      )\n",
    "\n",
    "      -- 4. Final Output Structure (The rest remains the same)\n",
    "      SELECT\n",
    "        t.transaction_date AS Date,\n",
    "        t.providerKey,\n",
    "        t.method,\n",
    "        t.channel_type,\n",
    "        t.type_formatted AS type,\n",
    "        t.reqCurrency,\n",
    "        account_group,\n",
    "        group_re,\n",
    "        \n",
    "        CASE t.reqCurrency\n",
    "          WHEN 'BDT' THEN 'Bangladesh'\n",
    "          WHEN 'THB' THEN 'Thailand'\n",
    "          WHEN 'MXN' THEN 'Mexico'\n",
    "          WHEN 'IDR' THEN 'Indonesia'\n",
    "          WHEN 'BRL' THEN 'Brazil'\n",
    "          WHEN 'PKR' THEN 'Pakistan'\n",
    "          WHEN 'INR' THEN 'India'\n",
    "          WHEN 'PHP' THEN 'Philippines'\n",
    "          ELSE 'Other'\n",
    "        END AS Country,\n",
    "        \n",
    "        t.status_formatted AS status,\n",
    "        t.Hour,\n",
    "        \n",
    "        COUNT(*) AS Count,\n",
    "        SUM(t.netAmount) AS Total_Net_Amount,\n",
    "        MAX(t.insertedAt) AS Max_InsertedAt, -- This will be the new watermark\n",
    "        \n",
    "        ROUND(SUM(\n",
    "          CASE \n",
    "            WHEN t.status_formatted = 'completed' AND t.transaction_time IS NOT NULL THEN\n",
    "              LEAST(GREATEST(t.transaction_time, s.p05), s.p95)\n",
    "            ELSE NULL \n",
    "          END\n",
    "        ), 2) AS winsorized_total_time_seconds,\n",
    "        \n",
    "        DATE_TRUNC(t.transaction_date, MONTH) AS DateMonth,\n",
    "\n",
    "        COUNTIF(t.status_formatted = 'completed' AND t.transaction_time <= 90) AS Count_01m30s_Below,\n",
    "        COUNTIF(t.status_formatted = 'completed' AND t.transaction_time > 90 AND t.transaction_time <= 120) AS Count_01m31s_to_02m00s,\n",
    "        COUNTIF(t.status_formatted = 'completed' AND t.transaction_time > 120 AND t.transaction_time <= 180) AS Count_02m01s_to_03m00s,\n",
    "        COUNTIF(t.status_formatted = 'completed' AND t.transaction_time > 180) AS Count_03m00s_Above,\n",
    "\n",
    "        COUNTIF(t.status_formatted = 'completed' AND t.transaction_time <= 180) AS Count_03m00s_Below,\n",
    "        COUNTIF(t.status_formatted = 'completed' AND t.transaction_time > 180 AND t.transaction_time <= 300) AS Count_03m31s_to_05m00s,\n",
    "        COUNTIF(t.status_formatted = 'completed' AND t.transaction_time > 300 AND t.transaction_time <= 600) AS Count_05m00s_to_10m00s,\n",
    "        COUNTIF(t.status_formatted = 'completed' AND t.transaction_time > 600) AS Count_10m00s_Above,\n",
    "\n",
    "        t.providerKey AS providerName,\n",
    "        SPLIT(t.channel_type, '-')[SAFE_OFFSET(0)] AS channel_main,\n",
    "\n",
    "          -- from mapping table\n",
    "        t.brand,\n",
    "\n",
    "      FROM all_transactions t\n",
    "      LEFT JOIN quantile_stats s\n",
    "        ON t.transaction_date = s.transaction_date\n",
    "        AND t.providerKey = s.providerKey\n",
    "        AND t.method = s.method\n",
    "        AND t.channel_type = s.channel_type\n",
    "        AND t.type_formatted = s.type_formatted\n",
    "        AND t.reqCurrency = s.reqCurrency\n",
    "        AND t.Hour = s.Hour\n",
    "\n",
    "      GROUP BY \n",
    "    t.transaction_date,\n",
    "    t.providerKey,\n",
    "    t.method,\n",
    "    t.channel_type,\n",
    "    t.type_formatted,\n",
    "    t.reqCurrency,\n",
    "    t.account_group,\n",
    "    group_re,\n",
    "    Country,\n",
    "    t.status_formatted,\n",
    "    t.Hour,\n",
    "    DateMonth,\n",
    "    t.providerKey,\n",
    "    channel_main,\n",
    "    t.brand\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Executing FULL historical query (first run)...\")\n",
    "\n",
    "    try:\n",
    "        df_full = client.query(sql_query).to_dataframe()\n",
    "    except NotFound as e:\n",
    "        print(f\"BigQuery Error: {e}\")\n",
    "        return None\n",
    "\n",
    "    if df_full.empty:\n",
    "        print(\"Query returned 0 rows.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Query returned {len(df_full)} rows (full load).\")\n",
    "\n",
    "    # Overwrite CSV for first load\n",
    "    df_full.to_csv(output_filename, mode='w', header=True, index=False)\n",
    "    print(f\"Successfully wrote full data to: {os.path.abspath(output_filename)}\")\n",
    "\n",
    "    return df_full\n",
    "\n",
    "\n",
    "# --- Google Sheets Writer Function (same logic, works for first run too) ---\n",
    "\n",
    "def append_df_to_gsheet_smart(dataframe: pd.DataFrame, worksheet_name: str):\n",
    "    if dataframe.empty:\n",
    "        print(f\"DataFrame for '{worksheet_name}' is empty. 0 rows appended.\")\n",
    "        return\n",
    "\n",
    "    scopes = [\n",
    "        'https://www.googleapis.com/auth/spreadsheets',\n",
    "        'https://www.googleapis.com/auth/drive'\n",
    "    ]\n",
    "    credentials = Credentials.from_service_account_file(CREDENTIALS_FILE, scopes=scopes)\n",
    "    gc = gspread.authorize(credentials)\n",
    "    \n",
    "    sh = gc.open_by_key(SHEET_ID)\n",
    "    \n",
    "    # Get or create worksheet\n",
    "    try:\n",
    "        worksheet = sh.worksheet(worksheet_name)\n",
    "    except gspread.WorksheetNotFound:\n",
    "        worksheet = sh.add_worksheet(title=worksheet_name, rows=\"1\", cols=\"1\")\n",
    "\n",
    "    # Determine next row\n",
    "    current_values = worksheet.get_all_values()\n",
    "    next_row = len(current_values) + 1\n",
    "    include_headers = (next_row == 1)\n",
    "\n",
    "    set_with_dataframe(\n",
    "        worksheet,\n",
    "        dataframe,\n",
    "        row=next_row,\n",
    "        col=1,\n",
    "        include_index=False,\n",
    "        include_column_header=include_headers,\n",
    "        resize=True\n",
    "    )\n",
    "\n",
    "    print(f\"Successfully appended {len(dataframe)} rows to '{worksheet_name}'.\")\n",
    "\n",
    "\n",
    "# --- Main Execution Block (FIRST RUN) ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Run the full query and get the data\n",
    "    full_data_df = run_full_query_and_save(\"daily_funding_full.csv\")\n",
    "\n",
    "    if full_data_df is not None and not full_data_df.empty:\n",
    "        print(\"\\nStarting Google Sheets upload process...\")\n",
    "    else:\n",
    "        print(\"\\nNo data retrieved from BigQuery for Google Sheets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfb249d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'daily_funding_full.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m full_data_df= \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdaily_funding_full.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Career\\Working\\KZ GROUP\\Data Synchronization\\PGW Health Dashboard\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Career\\Working\\KZ GROUP\\Data Synchronization\\PGW Health Dashboard\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Career\\Working\\KZ GROUP\\Data Synchronization\\PGW Health Dashboard\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Career\\Working\\KZ GROUP\\Data Synchronization\\PGW Health Dashboard\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Career\\Working\\KZ GROUP\\Data Synchronization\\PGW Health Dashboard\\venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'daily_funding_full.csv'"
     ]
    }
   ],
   "source": [
    "full_data_df= pd.read_csv('daily_funding_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fe01ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_merge = pd.read_csv('mapping_brand.csv')\n",
    "brand_merge = brand_merge.drop_duplicates(\"brand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7758e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = full_data_df[['Date', 'providerKey', 'method', 'channel_type', 'type', 'reqCurrency',\n",
    "       'account_group', 'Country', 'status', 'Hour', 'Count',\n",
    "       'Total_Net_Amount', 'Max_InsertedAt', 'winsorized_total_time_seconds',\n",
    "       'DateMonth', 'Count_01m30s_Below', 'Count_01m31s_to_02m00s',\n",
    "       'Count_02m01s_to_03m00s', 'Count_03m00s_Above', 'Count_03m00s_Below',\n",
    "       'Count_03m31s_to_05m00s', 'Count_05m00s_to_10m00s',\n",
    "       'Count_10m00s_Above', 'providerName', 'channel_main', 'brand']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3369bab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full_merge = data_full.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c1795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full_merge[\"brand\"] = data_full_merge[\"brand\"].str.upper().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c7653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full_merge = data_full_merge.merge(brand_merge, on = \"brand\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4a1e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full_merge = data_full_merge[data_full_merge[\"whitelabel\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08f7783",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deposit = data_full_merge[(data_full_merge['type'] == 'DEPOSIT')&(data_full_merge[\"Date\"]>='2025-10-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0469d6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_withdraw = data_full_merge[(data_full_merge['type'] == 'WITHDRAW')&(data_full_merge[\"Date\"]>='2025-10-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9414364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Google Sheets Writer Function ---\n",
    "def append_df_to_gsheet(dataframe: pd.DataFrame, worksheet_name: str, SHEET_ID: str):\n",
    "    \n",
    "    if dataframe.empty:\n",
    "        print(f\"DataFrame for '{worksheet_name}' is empty. 0 rows appended.\")\n",
    "        return\n",
    "\n",
    "    # --- AUTHENTICATION ---\n",
    "    scopes = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']\n",
    "    credentials = Credentials.from_service_account_file(CREDENTIALS_FILE, scopes=scopes)\n",
    "    gc = gspread.authorize(credentials)\n",
    "    \n",
    "    sh = gc.open_by_key(SHEET_ID)\n",
    "    \n",
    "    # Get or Create Worksheet\n",
    "    try:\n",
    "        worksheet = sh.worksheet(worksheet_name)\n",
    "    except gspread.WorksheetNotFound:\n",
    "        worksheet = sh.add_worksheet(title=worksheet_name, rows=\"1\", cols=\"1\")\n",
    "\n",
    "    # --- CALCULATE POSITION ---\n",
    "    current_values = worksheet.get_all_values()\n",
    "    # If sheet is empty, next_row is 1. If has data, it's len + 1\n",
    "    next_row = len(current_values) + 1\n",
    "    \n",
    "    # Write headers only if the sheet is brand new (row 1)\n",
    "    include_headers = (next_row == 1)\n",
    "\n",
    "    # --- WRITE DATA ---\n",
    "    set_with_dataframe(\n",
    "        worksheet, \n",
    "        dataframe, \n",
    "        row=next_row, \n",
    "        col=1, \n",
    "        include_index=False, \n",
    "        include_column_header=include_headers,\n",
    "        resize=True\n",
    "    )\n",
    "    \n",
    "    # --- PRINT ROW COUNT ---\n",
    "    # The number of data rows appended is simply the length of the dataframe\n",
    "    print(f\"Successfully appended {len(dataframe)} rows to '{worksheet_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ac236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GLOBAL CONFIGURATION (Update these variables) ---\n",
    "CREDENTIALS_FILE = 'gen-lang-client-0209575391-96d90a513b0b.json' # Path to your service account JSON key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a78f1bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIError",
     "evalue": "APIError: [400]: Invalid requests[0].updateSheetProperties: This action would increase the number of cells in the workbook above the limit of 10000000 cells.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAPIError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mappend_df_to_gsheet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_deposit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDEPOSIT_DATA\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m1hHV98ZAqng6ogy48iKDTP_tMVP8siwzgB8ybLSq40kU\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mappend_df_to_gsheet\u001b[39m\u001b[34m(dataframe, worksheet_name, SHEET_ID)\u001b[39m\n\u001b[32m     27\u001b[39m include_headers = (next_row == \u001b[32m1\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# --- WRITE DATA ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mset_with_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mworksheet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnext_row\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_column_header\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     38\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# --- PRINT ROW COUNT ---\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# The number of data rows appended is simply the length of the dataframe\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSuccessfully appended \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataframe)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows to \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworksheet_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Career\\Working\\KZ GROUP\\Data Synchronization\\PGW Health Dashboard\\venv\\Lib\\site-packages\\gspread_dataframe.py:348\u001b[39m, in \u001b[36mset_with_dataframe\u001b[39m\u001b[34m(worksheet, dataframe, row, col, include_index, include_column_header, resize, allow_formulas, string_escaping)\u001b[39m\n\u001b[32m    346\u001b[39m     x += col - \u001b[32m1\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resize:\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m     \u001b[43mworksheet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    350\u001b[39m     _resize_to_minimum(worksheet, y, x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Career\\Working\\KZ GROUP\\Data Synchronization\\PGW Health Dashboard\\venv\\Lib\\site-packages\\gspread\\worksheet.py:1523\u001b[39m, in \u001b[36mWorksheet.resize\u001b[39m\u001b[34m(self, rows, cols)\u001b[39m\n\u001b[32m   1507\u001b[39m fields = \u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33m\"\u001b[39m\u001b[33mgridProperties/\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m grid_properties.keys())\n\u001b[32m   1509\u001b[39m body = {\n\u001b[32m   1510\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrequests\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1511\u001b[39m         {\n\u001b[32m   (...)\u001b[39m\u001b[32m   1520\u001b[39m     ]\n\u001b[32m   1521\u001b[39m }\n\u001b[32m-> \u001b[39m\u001b[32m1523\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mspreadsheet_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1524\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m rows \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1525\u001b[39m     \u001b[38;5;28mself\u001b[39m._properties[\u001b[33m\"\u001b[39m\u001b[33mgridProperties\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mrowCount\u001b[39m\u001b[33m\"\u001b[39m] = rows\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Career\\Working\\KZ GROUP\\Data Synchronization\\PGW Health Dashboard\\venv\\Lib\\site-packages\\gspread\\http_client.py:139\u001b[39m, in \u001b[36mHTTPClient.batch_update\u001b[39m\u001b[34m(self, id, body)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbatch_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mid\u001b[39m: \u001b[38;5;28mstr\u001b[39m, body: Optional[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]]) -> Any:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Lower-level method that directly calls `spreadsheets/<ID>:batchUpdate <https://developers.google.com/sheets/api/reference/rest/v4/spreadsheets/batchUpdate>`_.\u001b[39;00m\n\u001b[32m    132\u001b[39m \n\u001b[32m    133\u001b[39m \u001b[33;03m    :param dict body: `Batch Update Request body <https://developers.google.com/sheets/api/reference/rest/v4/spreadsheets/batchUpdate#request-body>`_.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    137\u001b[39m \u001b[33;03m    .. versionadded:: 3.0\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     r = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSPREADSHEET_BATCH_UPDATE_URL\u001b[49m\u001b[43m \u001b[49m\u001b[43m%\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Career\\Working\\KZ GROUP\\Data Synchronization\\PGW Health Dashboard\\venv\\Lib\\site-packages\\gspread\\http_client.py:128\u001b[39m, in \u001b[36mHTTPClient.request\u001b[39m\u001b[34m(self, method, endpoint, params, data, json, files, headers)\u001b[39m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIError(response)\n",
      "\u001b[31mAPIError\u001b[39m: APIError: [400]: Invalid requests[0].updateSheetProperties: This action would increase the number of cells in the workbook above the limit of 10000000 cells."
     ]
    }
   ],
   "source": [
    "append_df_to_gsheet(df_deposit, 'DEPOSIT_DATA', '1hHV98ZAqng6ogy48iKDTP_tMVP8siwzgB8ybLSq40kU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041fab34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing data file: daily_funding_full.csv\n",
      "Found existing watermark: 2026-01-05 09:41:28.039808+00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Career\\Working\\KZ GROUP\\Data Synchronization\\PGW Health Dashboard\\venv\\Lib\\site-packages\\google\\auth\\_default.py:108: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "d:\\Career\\Working\\KZ GROUP\\Data Synchronization\\PGW Health Dashboard\\venv\\Lib\\site-packages\\google\\auth\\_default.py:108: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing incremental query since: 2026-01-05 09:41:28.039808+00:00\n",
      "Query returned 24441 new rows.\n",
      "Successfully appended new data to: d:\\Career\\Working\\KZ GROUP\\Data Synchronization\\PGW Health Dashboard\\daily_funding_full.csv\n",
      "\n",
      "Starting Google Sheets upload process...\n"
     ]
    }
   ],
   "source": [
    "# --- GLOBAL CONFIGURATION (Update these variables) ---\n",
    "CREDENTIALS_FILE = 'gen-lang-client-0209575391-96d90a513b0b.json' # Path to your service account JSON key\n",
    "SHEET_ID = '1Nq9u4bg0tvLnUutVh2TcxXOxe-G2E65kxy_pbJ8pce4' # The ID of your target Google Sheet\n",
    "# WORKSHEET_DEPOSIT_NAME = 'Deposit Data' \n",
    "# WORKSHEET_WITHDRAW_NAME = 'Withdrawal Data' \n",
    "# --------------------------------------------------\n",
    "\n",
    "\n",
    "# --- Helper Function to Get Watermark ---\n",
    "\n",
    "def get_last_run_watermark(output_filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads the existing CSV file to find the highest 'Max_InsertedAt' timestamp.\n",
    "    This timestamp will be used to filter the next BigQuery run.\n",
    "    \"\"\"\n",
    "    print(f\"Checking for existing data file: {output_filename}\")\n",
    "    \n",
    "    if not os.path.exists(output_filename):\n",
    "        print(\"Existing file not found. Performing full historical load.\")\n",
    "        # Return a very old timestamp to ensure full data load on the first run\n",
    "        return '1970-01-01 00:00:00+00' \n",
    "\n",
    "    try:\n",
    "        # We only need to load the column necessary for filtering\n",
    "        # Force reading as string first to handle mixed types/formats gracefully\n",
    "        df_existing = pd.read_csv(output_filename, usecols=['Max_InsertedAt'])\n",
    "        \n",
    "        # Ensure the column is datetime for accurate comparison\n",
    "        df_existing['Max_InsertedAt'] = pd.to_datetime(df_existing['Max_InsertedAt'], utc=True, errors='coerce')\n",
    "        df_existing.dropna(subset=['Max_InsertedAt'], inplace=True)\n",
    "\n",
    "        # Find the latest timestamp and format it for the SQL query\n",
    "        max_ts = df_existing['Max_InsertedAt'].max()\n",
    "        \n",
    "        # Format as a string with UTC timezone offset for BigQuery TIMESTAMP comparison\n",
    "        watermark = max_ts.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "        # BigQuery expects the +HH:MM format, but BigQuery is usually fine with the full string\n",
    "        watermark += '+00:00' \n",
    "        \n",
    "        print(f\"Found existing watermark: {watermark}\")\n",
    "        \n",
    "        return watermark\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading existing CSV file for watermark ({e}). Performing full load as fallback.\")\n",
    "        return '1970-01-01 00:00:00+00' \n",
    "\n",
    "\n",
    "# --- Incremental Query Function ---\n",
    "\n",
    "def run_incremental_query_and_save(output_filename='daily_funding_aggregation.csv'):\n",
    "    \"\"\"\n",
    "    Runs a BigQuery query incrementally, filtering for data inserted\n",
    "    after the last recorded Max_InsertedAt timestamp.\n",
    "    Appends new data to the local CSV file and returns the new DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Get the Watermark\n",
    "    last_inserted_at = get_last_run_watermark(output_filename)\n",
    "    \n",
    "    # 2. Initialize the BigQuery Client\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Define your SQL Query with a placeholder for the watermark\n",
    "    sql_query_template = \"\"\"\n",
    "     WITH raw_data AS (\n",
    "    SELECT\n",
    "      f.type, f.createdAt, f.completedAt, f.providerKey, f.method, f.status, f.reqCurrency, \n",
    "      f.accountId, f.netAmount, f.insertedAt, \n",
    "      SPLIT(f.method, '/')[SAFE_OFFSET(1)] AS channel_type,\n",
    "\n",
    "      -- mapping fields\n",
    "      a.name AS brand,\n",
    "\n",
    "      DATETIME(f.createdAt, CASE f.reqCurrency\n",
    "        WHEN 'BDT' THEN '+06:00'\n",
    "        WHEN 'THB' THEN 'Asia/Bangkok'\n",
    "        WHEN 'MXN' THEN 'America/Mexico_City'\n",
    "        WHEN 'IDR' THEN 'Asia/Jakarta'\n",
    "        WHEN 'BRL' THEN 'America/Sao_Paulo'\n",
    "        WHEN 'PKR' THEN 'Asia/Karachi'\n",
    "        WHEN 'INR' THEN '+05:30'\n",
    "        WHEN 'PHP' THEN 'Asia/Manila'\n",
    "        ELSE 'UTC'\n",
    "      END) AS local_ts,\n",
    "\n",
    "      CASE \n",
    "        WHEN f.status = 'completed' AND f.type = 'deposit' THEN\n",
    "          LEAST(TIMESTAMP_DIFF(f.completedAt, f.createdAt, SECOND), 900)\n",
    "        WHEN f.status = 'completed' AND f.type = 'withdraw' THEN\n",
    "          TIMESTAMP_DIFF(f.completedAt, f.createdAt, SECOND)\n",
    "        ELSE NULL\n",
    "      END AS transaction_time\n",
    "\n",
    "    FROM `kz-dp-prod.kz_pg_to_bq_realtime.ext_funding_tx` AS f\n",
    "    LEFT JOIN `kz-dp-prod.kz_pg_to_bq_realtime.account` AS a ON f.accountId = a.id\n",
    "    WHERE \n",
    "      f.insertedAt > TIMESTAMP('{last_inserted_at}') \n",
    "      AND f.type IN ('deposit', 'withdraw')\n",
    "      AND f.reqCurrency IN ('BDT', 'THB', 'MXN', 'IDR', 'BRL', 'PKR', 'INR', 'PHP')\n",
    "      AND f.status IN ('completed', 'errors', 'timeout', 'error')\n",
    "    \n",
    "    QUALIFY ROW_NUMBER() OVER (PARTITION BY DATE(local_ts), f.id ORDER BY f.updatedAt DESC) = 1\n",
    "  ),\n",
    "\n",
    "      -- 2. Enrich Dimensions & Join Account Info (No change needed here)\n",
    "    all_transactions AS (\n",
    "    SELECT\n",
    "      r.*,\n",
    "      CASE WHEN LEFT(a.group,3) = 'kzg' THEN 'KZG' ELSE 'KZP' END AS group_re,\n",
    "      UPPER(a.group) AS account_group,\n",
    "      DATE(r.local_ts) AS transaction_date,\n",
    "      DATE_TRUNC(DATE(r.local_ts), MONTH) AS transaction_month,\n",
    "      FORMAT_DATETIME('%H:00 - %H:59', r.local_ts) AS Hour,\n",
    "      UPPER(r.type) AS type_formatted, \n",
    "      CASE WHEN r.status = 'errors' THEN 'error' ELSE r.status END AS status_formatted\n",
    "    FROM raw_data r\n",
    "    LEFT JOIN `kz-dp-prod.kz_pg_to_bq_realtime.account` a \n",
    "      ON r.accountId = a.Id\n",
    "  ),\n",
    "      -- 3. Calculate Quantile Bounds (DAILY Stats) - Only for the new/updated transactions\n",
    "      quantile_stats AS (\n",
    "        SELECT\n",
    "          transaction_date,\n",
    "          providerKey,\n",
    "          method,\n",
    "          channel_type,\n",
    "          type_formatted,\n",
    "          reqCurrency,\n",
    "          Hour,\n",
    "          APPROX_QUANTILES(transaction_time, 101)[OFFSET(5)] AS p05,\n",
    "          APPROX_QUANTILES(transaction_time, 101)[OFFSET(50)] AS p50,\n",
    "          APPROX_QUANTILES(transaction_time, 101)[OFFSET(95)] AS p95\n",
    "        FROM all_transactions\n",
    "        WHERE status_formatted = 'completed' AND transaction_time IS NOT NULL\n",
    "        GROUP BY 1, 2, 3, 4, 5, 6, 7\n",
    "      )\n",
    "\n",
    "      -- 4. Final Output Structure (The rest remains the same)\n",
    "      SELECT\n",
    "        t.transaction_date AS Date,\n",
    "        t.providerKey,\n",
    "        t.method,\n",
    "        t.channel_type,\n",
    "        t.type_formatted AS type,\n",
    "        t.reqCurrency,\n",
    "        account_group,\n",
    "        group_re,\n",
    "        \n",
    "        CASE t.reqCurrency\n",
    "          WHEN 'BDT' THEN 'Bangladesh'\n",
    "          WHEN 'THB' THEN 'Thailand'\n",
    "          WHEN 'MXN' THEN 'Mexico'\n",
    "          WHEN 'IDR' THEN 'Indonesia'\n",
    "          WHEN 'BRL' THEN 'Brazil'\n",
    "          WHEN 'PKR' THEN 'Pakistan'\n",
    "          WHEN 'INR' THEN 'India'\n",
    "          WHEN 'PHP' THEN 'Philippines'\n",
    "          ELSE 'Other'\n",
    "        END AS Country,\n",
    "        \n",
    "        t.status_formatted AS status,\n",
    "        t.Hour,\n",
    "        \n",
    "        COUNT(*) AS Count,\n",
    "        SUM(t.netAmount) AS Total_Net_Amount,\n",
    "        MAX(t.insertedAt) AS Max_InsertedAt, -- This will be the new watermark\n",
    "        \n",
    "        ROUND(SUM(\n",
    "          CASE \n",
    "            WHEN t.status_formatted = 'completed' AND t.transaction_time IS NOT NULL THEN\n",
    "              LEAST(GREATEST(t.transaction_time, s.p05), s.p95)\n",
    "            ELSE NULL \n",
    "          END\n",
    "        ), 2) AS winsorized_total_time_seconds,\n",
    "        \n",
    "        DATE_TRUNC(t.transaction_date, MONTH) AS DateMonth,\n",
    "\n",
    "        COUNTIF(t.status_formatted = 'completed' AND t.transaction_time <= 90) AS Count_01m30s_Below,\n",
    "        COUNTIF(t.status_formatted = 'completed' AND t.transaction_time > 90 AND t.transaction_time <= 120) AS Count_01m31s_to_02m00s,\n",
    "        COUNTIF(t.status_formatted = 'completed' AND t.transaction_time > 120 AND t.transaction_time <= 180) AS Count_02m01s_to_03m00s,\n",
    "        COUNTIF(t.status_formatted = 'completed' AND t.transaction_time > 180) AS Count_03m00s_Above,\n",
    "\n",
    "        COUNTIF(t.status_formatted = 'completed' AND t.transaction_time <= 180) AS Count_03m00s_Below,\n",
    "        COUNTIF(t.status_formatted = 'completed' AND t.transaction_time > 180 AND t.transaction_time <= 300) AS Count_03m31s_to_05m00s,\n",
    "        COUNTIF(t.status_formatted = 'completed' AND t.transaction_time > 300 AND t.transaction_time <= 600) AS Count_05m00s_to_10m00s,\n",
    "        COUNTIF(t.status_formatted = 'completed' AND t.transaction_time > 600) AS Count_10m00s_Above,\n",
    "\n",
    "        t.providerKey AS providerName,\n",
    "        SPLIT(t.channel_type, '-')[SAFE_OFFSET(0)] AS channel_main,\n",
    "\n",
    "          -- from mapping table\n",
    "        t.brand,\n",
    "\n",
    "      FROM all_transactions t\n",
    "      LEFT JOIN quantile_stats s\n",
    "        ON t.transaction_date = s.transaction_date\n",
    "        AND t.providerKey = s.providerKey\n",
    "        AND t.method = s.method\n",
    "        AND t.channel_type = s.channel_type\n",
    "        AND t.type_formatted = s.type_formatted\n",
    "        AND t.reqCurrency = s.reqCurrency\n",
    "        AND t.Hour = s.Hour\n",
    "\n",
    "      GROUP BY \n",
    "    t.transaction_date,\n",
    "    t.providerKey,\n",
    "    t.method,\n",
    "    t.channel_type,\n",
    "    t.type_formatted,\n",
    "    t.reqCurrency,\n",
    "    t.account_group,\n",
    "    group_re,\n",
    "    Country,\n",
    "    t.status_formatted,\n",
    "    t.Hour,\n",
    "    DateMonth,\n",
    "    t.providerKey,\n",
    "    channel_main,\n",
    "    t.brand \n",
    "    \"\"\"\n",
    "    \n",
    "    # 3. Insert the Watermark into the Query\n",
    "    query = sql_query_template.format(last_inserted_at=last_inserted_at)\n",
    "    \n",
    "    print(f\"Executing incremental query since: {last_inserted_at}\")\n",
    "    \n",
    "    # 4. Execute Query and Download to DataFrame\n",
    "    try:\n",
    "        df_new = client.query(query).to_dataframe()\n",
    "    except NotFound as e:\n",
    "        print(f\"BigQuery Error: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Check if any new rows were returned\n",
    "    if df_new.empty:\n",
    "        print(\"Query returned 0 new rows. File remains unchanged.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Query returned {len(df_new)} new rows.\")\n",
    "\n",
    "    # 5. Save/Append to CSV (maintaining the watermark for next run)\n",
    "    \n",
    "    # Check if the file exists to determine if we need to write headers\n",
    "    file_exists = os.path.exists(output_filename)\n",
    "    \n",
    "    # Use 'a' (append mode) and set header=False if the file already exists\n",
    "    df_new.to_csv(output_filename, \n",
    "                  mode='a', \n",
    "                  header=not file_exists, \n",
    "                  index=False)\n",
    "    \n",
    "    print(f\"Successfully appended new data to: {os.path.abspath(output_filename)}\")\n",
    "    \n",
    "    # Return the new DataFrame for GSheet processing\n",
    "    return df_new\n",
    "\n",
    "\n",
    "# --- Google Sheets Writer Function ---\n",
    "def append_df_to_gsheet_smart(dataframe: pd.DataFrame, worksheet_name: str):\n",
    "    \n",
    "    if dataframe.empty:\n",
    "        print(f\"DataFrame for '{worksheet_name}' is empty. 0 rows appended.\")\n",
    "        return\n",
    "\n",
    "    # --- AUTHENTICATION ---\n",
    "    scopes = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']\n",
    "    credentials = Credentials.from_service_account_file(CREDENTIALS_FILE, scopes=scopes)\n",
    "    gc = gspread.authorize(credentials)\n",
    "    \n",
    "    sh = gc.open_by_key(SHEET_ID)\n",
    "    \n",
    "    # Get or Create Worksheet\n",
    "    try:\n",
    "        worksheet = sh.worksheet(worksheet_name)\n",
    "    except gspread.WorksheetNotFound:\n",
    "        worksheet = sh.add_worksheet(title=worksheet_name, rows=\"1\", cols=\"1\")\n",
    "\n",
    "    # --- CALCULATE POSITION ---\n",
    "    current_values = worksheet.get_all_values()\n",
    "    # If sheet is empty, next_row is 1. If has data, it's len + 1\n",
    "    next_row = len(current_values) + 1\n",
    "    \n",
    "    # Write headers only if the sheet is brand new (row 1)\n",
    "    include_headers = (next_row == 1)\n",
    "\n",
    "    # --- WRITE DATA ---\n",
    "    set_with_dataframe(\n",
    "        worksheet, \n",
    "        dataframe, \n",
    "        row=next_row, \n",
    "        col=1, \n",
    "        include_index=False, \n",
    "        include_column_header=include_headers,\n",
    "        resize=True\n",
    "    )\n",
    "    \n",
    "    # --- PRINT ROW COUNT ---\n",
    "    # The number of data rows appended is simply the length of the dataframe\n",
    "    print(f\"Successfully appended {len(dataframe)} rows to '{worksheet_name}'.\")\n",
    "\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Run the incremental query and get the new data\n",
    "    new_data_df = run_incremental_query_and_save(\"daily_funding_full.csv\")\n",
    "\n",
    "    if new_data_df is not None and not new_data_df.empty:\n",
    "        print(\"\\nStarting Google Sheets upload process...\")\n",
    "        \n",
    "        # 2. Split the DataFrame based on 'type' (which contains 'DEPOSIT' or 'WITHDRAW')\n",
    "        # df_deposit = new_data_df[new_data_df['type'] == 'DEPOSIT'].copy()\n",
    "        # df_withdraw = new_data_df[new_data_df['type'] == 'WITHDRAW'].copy()\n",
    "        \n",
    "        # We need to drop the 'Max_InsertedAt' column from the data being uploaded\n",
    "        # as it contains BigQuery timestamp data useful for the watermark but usually not for the report.\n",
    "        # if not df_deposit.empty:\n",
    "            # 3. Write DEPOSIT data to Google Sheet (APPENDS data now)\n",
    "            # append_df_to_gsheet_smart(df_deposit, WORKSHEET_DEPOSIT_NAME)\n",
    "        # else:\n",
    "            # print(f\"No new DEPOSIT records found in this run to upload to '{WORKSHEET_DEPOSIT_NAME}'.\")\n",
    "\n",
    "        # if not df_withdraw.empty:\n",
    "            # 4. Write WITHDRAW data to a separate sheet (APPENDS data now)\n",
    "            # append_df_to_gsheet_smart(df_withdraw, WORKSHEET_WITHDRAW_NAME)\n",
    "        # else:\n",
    "            # print(f\"No new WITHDRAW records found in this run to upload to '{WORKSHEET_WITHDRAW_NAME}'.\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nNo new data retrieved from BigQuery to process for Google Sheets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf3b1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
